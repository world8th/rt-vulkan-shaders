#version 460 core
#extension GL_GOOGLE_include_directive : enable

#include "./includes.glsl"

layout (local_size_x = BLOCK_SIZE) in;

shared uint localHistogram[RADICES];

// shared for 16 threads (with lanes)
#if defined(ENABLE_AMD_INSTRUCTION_SET) && defined(ENABLE_AMD_INT16)
shared uint_rdc_wave_lcm _data[64];
#else
shared lowp uint_rdc_wave_lcm _data[128];
#endif

#define key _data[Lane_Idx]

//initSubgroupIncFunctionTarget(localHistogram[WHERE], countOffset, 1, uint)

#if defined(ENABLE_AMD_INSTRUCTION_SET) && defined(ENABLE_AMD_INT16)
initSubgroupIncFunctionTargetDual(localHistogram[WHERE], countOffset, 1, uint, uvec2)
#else
initSubgroupIncFunctionTarget(localHistogram[WHERE], countOffset, 1, uint)
#endif

void main() {
    Local_Idx = gl_LocalInvocationID.x;
    Wave_Idx = gl_LocalInvocationID.x / Wave_Size_RT;
    Lane_Idx = gl_LocalInvocationID.x % Wave_Size_RT;
    Radice_Idx = gl_WorkGroupID.y + Wave_Idx * gl_NumWorkGroups.y;

    // set prefix sum (planned distribute threads) 
    [[unroll]]
    for (uint rk=0;rk<RADICES;rk+=WRK_SIZE_RT) {
        uvec_wave radice = uvec_wave(rk + Radice_Idx);
        localHistogram[radice] = PrefixSum[radice + gl_WorkGroupID.x * RADICES];
    }
    
    LGROUP_BARRIER

    // calculate blocks
    blocks_info blocks = get_blocks_info(NumKeys);

#if defined(ENABLE_AMD_INSTRUCTION_SET) && defined(ENABLE_AMD_INT16)
    uint bcount = min(tiled(blocks.count, 2u), 524288u);
    WPTR2 addr = WPTR(blocks.offset).xx + WPTR2(Lane_Idx, Wave_Size_RT + Lane_Idx);
#else
    uint bcount = min(blocks.count, 524288u);
    WPTR addr = WPTR(blocks.offset) + WPTR(Lane_Idx);
#endif

    for ( uint wk = 0; wk < bcount; wk++ ) {
#if defined(ENABLE_AMD_INSTRUCTION_SET) && defined(ENABLE_AMD_INT16)
        bvec2_wave validAddress = lessThan(addr, blocks.limit.xx);
        IFALL(all(not(validAddress))) break;
#else
        bvec_wave validAddress = addr < blocks.limit;
        IFALL(!validAddress.x) break;
#endif

        if (Wave_Idx == 0) {
#if defined(ENABLE_AMD_INSTRUCTION_SET) && defined(ENABLE_AMD_INT16)
            key = packUint2x16(uint_rdc_wave_2(
                BFE(validAddress.x ? KeyIn[addr.x] : KEYTYPE(0xFFFFFFFFu.xx), Shift*BITS_PER_PASS, BITS_PER_PASS),
                BFE(validAddress.y ? KeyIn[addr.y] : KEYTYPE(0xFFFFFFFFu.xx), Shift*BITS_PER_PASS, BITS_PER_PASS)
            ));
#else
            key.x = uint_rdc_wave(BFE(validAddress.x ? KeyIn[addr.x] : KEYTYPE(0xFFFFFFFFu.xx), Shift*BITS_PER_PASS, BITS_PER_PASS));
#endif
        }
        LGROUP_BARRIER

        // WARP-optimized histogram calculation
        for (uint rk=0u;rk<RADICES;rk+=WRK_SIZE_RT) {
            uvec_wave radice = uvec_wave(rk + Radice_Idx);
#if defined(ENABLE_AMD_INSTRUCTION_SET) && defined(ENABLE_AMD_INT16)
            bvec2 owned = and(equal(unpackUint2x16(key), radice.xx), validAddress);
            if (any(owned)) {
                WPTR2 offset = WPTR2(countOffset(uint(radice), owned));
                if (owned.x) { ValueTmp[offset.x] = ValueIn[addr.x], KeyTmp[offset.x] = KeyIn[addr.x]; }
                if (owned.y) { ValueTmp[offset.y] = ValueIn[addr.y], KeyTmp[offset.y] = KeyIn[addr.y]; }
            }
            IFALL (all(or((radice >= RADICES).xx, or(owned, not(validAddress))))) break;
#else
            bool owned = key.x == radice && validAddress;
            if (owned) {
                WPTR offset = WPTR(countOffset(uint(radice))); 
                ValueTmp[offset.x] = ValueIn[addr.x];
                KeyTmp[offset.x] = KeyIn[addr.x];
            }
            IFALL (radice >= RADICES || owned || !validAddress) break;
#endif
        }

#ifdef ENABLE_AMD_INSTRUCTION_SET
        addr += Wave_Size_RT*2;
#else
        addr += Wave_Size_RT;
#endif
    }

    LGROUP_BARRIER
}
