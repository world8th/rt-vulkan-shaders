#version 460 core
#extension GL_GOOGLE_include_directive : enable

#ifdef NVIDIA_PLATFORM
#define WORK_SIZE_BND 1024
#else 
#define WORK_SIZE_BND 1024
#endif

#define BVH_BUILD
#define BVH_CREATION

#include "../include/driver.glsl"
#include "../include/structs.glsl"
#include "../include/uniforms.glsl"
#include "../include/mathlib.glsl"
#include "../include/ballotlib.glsl"
#include "../include/vertex.glsl"
#include "./includes.glsl"

// shared memory counters
shared int _counters[8];
#define cBuffer _counters[3]

// define function for increment
initAtomicSubgroupIncFunction(_counters[0], aCounterInc, 1, int)
initAtomicSubgroupIncFunction(_counters[1], lCounterInc, 2, int)
initAtomicSubgroupIncFunction(_counters[2], cCounterInc, 1, int)
initAtomicSubgroupIncFunction(_counters[2], cCounterDualInc, 2, int)

#include "./bvh-build-general.glsl" // unified functions

const int largeStageThreshold = 2;

layout ( local_size_x = WORK_SIZE_BND ) in;

void main() {
    const int threadID = int(Local_Idx);
    const int groupSize = int(gl_WorkGroupSize.x);

    // lane-based
    const int gS = groupSize >> 1;
    const int iT = threadID >> 1;
    const int sD = threadID & 1;

    ///const int gS = groupSize;
    //const int iT = threadID;
    //const int sD = 0;

    LGROUP_BARRIER
    if (threadID < 8) { _counters[threadID] = 0; }
    LGROUP_BARRIER

    // create initial (root) node
    if (threadID == 0) {
        int hid = lCounterInc();
        bvhMeta[hid+0] = ivec4(1, bvhBlock.creatorUniform.leafCount, 0, 0);
        bvhMeta[hid+1] = ivec4(0, 0, 0, 0);
        Actives[aCounterInc()][cBuffer] = hid+1;
    }
    LGROUP_BARRIER
    
    // building BVH
    [[unroll, dependency_length(4)]]
    for (int m=0;m<65536;m++) {
        // check activity counter
        const int asize = _counters[0];
        IFALL (asize <= 0) break; // if there is no element, stop splitting

        // go to large stages
        IFALL (asize >= int(gl_WorkGroupSize.x)*largeStageThreshold) {
            IFANY (cBuffer == 1) {
                // if swap index = 1, copy elements for economy pushing constants
                for (int fT=0;fT<asize;fT+=gS*2) {
                    const int uID = fT + threadID; // fit for every threads, don't compact here
                    Actives[uID][0] = Actives[uID][1];
                }
            }
            break;
        }

        // switch buffer and reset counter
        LGROUP_BARRIER
        if (threadID == 0) { cBuffer = 1-cBuffer; _counters[0] = 0; }
        LGROUP_BARRIER

        // split nodes
        for (int fT=0;fT<asize;fT+=gS) {
            // subgroup barrier
            SB_BARRIER

            // index of node element
            const int uID = fT + iT; 
            
            // split prefixed elements
            IFALL (uID >= asize) break;

            // get spared prefix
            const int fID = Actives[uID][1-cBuffer]-1;
            if (sD == 0) Actives[uID][1-cBuffer] = 0;

            // split sibling nodes
            [[flatten]]
            //if (uID < asize && fID >= 0) { splitNode(fID, 0), splitNode(fID, 1); }
            if (uID < asize && fID >= 0) { splitNode(fID, sD); }
        }

        // sync BVH splitting
        LGROUP_BARRIER
    }

    // copy to external counters
    if (threadID == 0) {
        aCounter = _counters[0];
        //aCounter = 0;
        lCounter = _counters[1];
        cCounter = _counters[2];
        aCounter2 = 0;
        //aCounter2 = _counters[0];
    }
    LGROUP_BARRIER
}
